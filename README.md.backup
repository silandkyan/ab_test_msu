# ab_test_msu

## Introduction

The Library of Montana State University has a website that students use to find books and articles. 

Below the library picture, there is a search bar and three big items: “Find”, “Request” and “Interact”. All three of them contain access to important information and services that the library prides itself in offering. However, the Website Analytics show that the “Interact” button has, ironically, almost no interactions.

The way to measure how each one of the three categories performs is by click-through rate (CTR). The report from the team analysing the website shows the specific numbers and explains how they reacted to them:

> During the sample period from April 3, 2013 – April 10, 2013, which included 10,819 visits to the library homepage, there was a large disparity among the three main content categories. The click-through rate for Find was 35%, Request was 6%, and Interact was 2%. This observation prompted a question: “Why are Interact clicks so low?” At this time, the content beneath Interact included links to Reference Services, Instruction Services, Subject Liaisons, Writing Center, About, Staff Directory, Library FAQ, Give to the Library, and Floor Maps. The library’s web committee surmised that introducing this category with the abstract term “Interact” added difficulty and confusion for users trying to navigate into the library website homepage. Four different category titles were then proposed as variations to be tested: Connect, Learn, Help, and Services.

In preparation for an A/B Test, the UX team had conversations with a few students. One goal was to get opinions on the category titles proposed to replace "Interact". Most students favoured the terms "Help" and "Services" over "Connect" and "Learn".


## Experiment design

* Include all proposed variants.
* Added business value would be to improve the visibility of Library services to the students.
* Main metric: Click-through rate (CTR)
* Additional metrics: Drop-off rate and Homepage-return rate for the category pages
* Since there is a lot of room for improvement, the minimum detectable effect should be set large, e.g. 30% increase in CTR.
* Use external tool for experiment setup.

The hypotheses to be tested in the experiment are the following:

* Null Hypothesis: all versions have the same CTR.
* Alternative Hypothesis: there is a difference in the CTR for the different versions.

A minimum increase in click-through rate of 30% has to be detected.

Desired statistical significance: 90%

Length of the experiment: 21 days (calculated based on current CTR for Interact of 2% and ca. 1650 daily page visitors)